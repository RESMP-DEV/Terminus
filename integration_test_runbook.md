# Integration Test Runbook

This document outlines the steps required to run a full end-to-end integration test of the agent, using the local `gpt-oss-20b` model for command execution.

## Prerequisites

1.  **Install Dependencies**: Ensure all project dependencies are installed by running the following commands:
    ```bash
    pip install -r requirements.txt
    pip install -r requirements-dev.txt
    pip install -e .
    ```

2.  **Configure Environment**: Create a `.env` file from the `.env.example` and ensure the `EXECUTOR_API_URL` is correctly configured (the default `http://localhost:8002/generate` should be sufficient).

## Running the Test

The integration test requires two main components to be running concurrently: the local executor API server and the main agent application.

### Step 1: Start the Local Executor Server

In a terminal window, start the FastAPI server that hosts the command generation model:

```bash
python server.py
```

The server will start on `localhost:8002`. You should see log output indicating that the model is being loaded.

### Step 2: Run the Main Application

In a separate terminal window, run the main agent application. You can use the provided demo script for this:

```bash
python scripts/run_demo.py
```

### Step 3: Observe the Workflow

The `run_demo.py` script will simulate a user goal. You should observe the following sequence of events in the logs of the main application:

1.  A plan is generated by the planner model (GPT-5).
2.  For each step in the plan, the orchestrator calls the `run_executor` function.
3.  The `run_executor` function sends a POST request to the local server at `http://localhost:8002/generate`.
4.  The local server responds with a JSON object containing the generated bash command.
5.  The command is executed in the sandbox.
6.  The process repeats for all steps in the plan.

If all steps complete successfully, the integration test is considered a success.